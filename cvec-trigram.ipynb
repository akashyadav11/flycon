{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df['sentiment'] = df['sentiment'].map({0:0,4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text'] #define all other columns except the target variable\n",
    "y = df['sentiment'] #define the target variable\n",
    "\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size = 0.02, random_state = 42)\n",
    "\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, \n",
    "                                                              test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_accuracy = 0\n",
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test==0])/len(x_test)>0.5:\n",
    "        null_accuracy = len(x_test[y_test==0])/len(x_test)\n",
    "    else:\n",
    "        null_accuracy = 1 - len(x_test[y_test==0])/len(x_test)\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    report=classification_report(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print(\"Accuracy: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy>null_accuracy:\n",
    "        print(\"Model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy==null_accuracy:\n",
    "        print(\"Model has the same accuracy as null accuracy\")\n",
    "    else:\n",
    "        print(\"Model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print(\"Train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print(\"-\"*50)\n",
    "    print(report)\n",
    "    return accuracy, train_test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(10000, 100001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfeature_accuracy_checker(vectorizer = cvec, n_features = n_features, stop_words = None, \n",
    "                              ngram_range = (1,1), classifier = lr):\n",
    "    result = []\n",
    "    print(classifier, \"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words = stop_words, max_features = n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "        print(\"Validation result for {0} features\".format(n))\n",
    "        nfeature_accuracy, ttime = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n, nfeature_accuracy, ttime))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Ridge Classifier', 'Logistic Regression', 'Perceptron', 'Passive-Agressive Classifier', 'Stochastic Gradient Descent',\n",
    "         'LinearSVC', 'L1 based LinearSVC', 'KNN', 'Nearest Centroid', 'Multinomial NB', 'Bernoulli NB', 'Adaboost']\n",
    "classifiers = [\n",
    "    RidgeClassifier(),\n",
    "    LogisticRegression(),\n",
    "    Perceptron(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    SGDClassifier(),\n",
    "    LinearSVC(),\n",
    "    Pipeline([\n",
    "        ('feature_selection', SelectFromModel(LinearSVC(penalty='l1', dual=False))),\n",
    "        ('classification', LinearSVC(penalty='l2'))\n",
    "    ]),\n",
    "    KNeighborsClassifier(),\n",
    "    NearestCentroid(),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    AdaBoostClassifier(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_clf = zip(names, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_comparator(vectorizer = cvec, n_features=10000, stop_words=None, ngram_range=(1,1), classifier=zipped_clf):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=stop_words, ngram_range=ngram_range, max_features=n_features)\n",
    "    for n, c in classifier:\n",
    "        pipeline = Pipeline([('vectorizer', vectorizer), ('classifier', c)])\n",
    "        print('Validation result for {}'.format(n), c)\n",
    "        clf_accuracy, ttime = accuracy_summary(pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        \n",
    "        result.append((n, clf_accuracy, ttime))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Ridge Classifier RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "                max_iter=None, normalize=False, random_state=None,\n",
      "                solver='auto', tol=0.001)\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 81.86%\n",
      "Model is 31.67% more accurate than null accuracy\n",
      "Train and test time: 580.60s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.82      8013\n",
      "           1       0.81      0.84      0.82      7954\n",
      "\n",
      "    accuracy                           0.82     15967\n",
      "   macro avg       0.82      0.82      0.82     15967\n",
      "weighted avg       0.82      0.82      0.82     15967\n",
      "\n",
      "Validation result for Logistic Regression LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null accuracy: 50.18%\n",
      "Accuracy: 82.38%\n",
      "Model is 32.19% more accurate than null accuracy\n",
      "Train and test time: 611.43s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      8013\n",
      "           1       0.82      0.83      0.83      7954\n",
      "\n",
      "    accuracy                           0.82     15967\n",
      "   macro avg       0.82      0.82      0.82     15967\n",
      "weighted avg       0.82      0.82      0.82     15967\n",
      "\n",
      "Validation result for Perceptron Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
      "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
      "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
      "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 75.76%\n",
      "Model is 25.57% more accurate than null accuracy\n",
      "Train and test time: 750.20s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75      8013\n",
      "           1       0.74      0.78      0.76      7954\n",
      "\n",
      "    accuracy                           0.76     15967\n",
      "   macro avg       0.76      0.76      0.76     15967\n",
      "weighted avg       0.76      0.76      0.76     15967\n",
      "\n",
      "Validation result for Passive-Agressive Classifier PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "                            early_stopping=False, fit_intercept=True,\n",
      "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
      "                            n_jobs=None, random_state=None, shuffle=True,\n",
      "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                            warm_start=False)\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 75.93%\n",
      "Model is 25.75% more accurate than null accuracy\n",
      "Train and test time: 186.06s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77      8013\n",
      "           1       0.79      0.70      0.74      7954\n",
      "\n",
      "    accuracy                           0.76     15967\n",
      "   macro avg       0.76      0.76      0.76     15967\n",
      "weighted avg       0.76      0.76      0.76     15967\n",
      "\n",
      "Validation result for Stochastic Gradient Descent SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 81.39%\n",
      "Model is 31.20% more accurate than null accuracy\n",
      "Train and test time: 183.68s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81      8013\n",
      "           1       0.80      0.84      0.82      7954\n",
      "\n",
      "    accuracy                           0.81     15967\n",
      "   macro avg       0.81      0.81      0.81     15967\n",
      "weighted avg       0.81      0.81      0.81     15967\n",
      "\n",
      "Validation result for LinearSVC LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null accuracy: 50.18%\n",
      "Accuracy: 82.06%\n",
      "Model is 31.88% more accurate than null accuracy\n",
      "Train and test time: 827.04s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      8013\n",
      "           1       0.81      0.84      0.82      7954\n",
      "\n",
      "    accuracy                           0.82     15967\n",
      "   macro avg       0.82      0.82      0.82     15967\n",
      "weighted avg       0.82      0.82      0.82     15967\n",
      "\n",
      "Validation result for L1 based LinearSVC Pipeline(memory=None,\n",
      "         steps=[('feature_selection',\n",
      "                 SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None,\n",
      "                                                     dual=False,\n",
      "                                                     fit_intercept=True,\n",
      "                                                     intercept_scaling=1,\n",
      "                                                     loss='squared_hinge',\n",
      "                                                     max_iter=1000,\n",
      "                                                     multi_class='ovr',\n",
      "                                                     penalty='l1',\n",
      "                                                     random_state=None,\n",
      "                                                     tol=0.0001, verbose=0),\n",
      "                                 max_features=None, norm_order=1, prefit=False,\n",
      "                                 threshold=None)),\n",
      "                ('classification',\n",
      "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
      "                           fit_intercept=True, intercept_scaling=1,\n",
      "                           loss='squared_hinge', max_iter=1000,\n",
      "                           multi_class='ovr', penalty='l2', random_state=None,\n",
      "                           tol=0.0001, verbose=0))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null accuracy: 50.18%\n",
      "Accuracy: 82.14%\n",
      "Model is 31.95% more accurate than null accuracy\n",
      "Train and test time: 1220.14s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      8013\n",
      "           1       0.81      0.84      0.82      7954\n",
      "\n",
      "    accuracy                           0.82     15967\n",
      "   macro avg       0.82      0.82      0.82     15967\n",
      "weighted avg       0.82      0.82      0.82     15967\n",
      "\n",
      "Validation result for KNN KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 71.87%\n",
      "Model is 21.69% more accurate than null accuracy\n",
      "Train and test time: 1968.04s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.66      0.70      8013\n",
      "           1       0.69      0.78      0.73      7954\n",
      "\n",
      "    accuracy                           0.72     15967\n",
      "   macro avg       0.72      0.72      0.72     15967\n",
      "weighted avg       0.72      0.72      0.72     15967\n",
      "\n",
      "Validation result for Nearest Centroid NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 63.78%\n",
      "Model is 13.60% more accurate than null accuracy\n",
      "Train and test time: 287.85s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.57      0.61      8013\n",
      "           1       0.62      0.70      0.66      7954\n",
      "\n",
      "    accuracy                           0.64     15967\n",
      "   macro avg       0.64      0.64      0.64     15967\n",
      "weighted avg       0.64      0.64      0.64     15967\n",
      "\n",
      "Validation result for Multinomial NB MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 79.73%\n",
      "Model is 29.55% more accurate than null accuracy\n",
      "Train and test time: 251.46s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      8013\n",
      "           1       0.80      0.79      0.80      7954\n",
      "\n",
      "    accuracy                           0.80     15967\n",
      "   macro avg       0.80      0.80      0.80     15967\n",
      "weighted avg       0.80      0.80      0.80     15967\n",
      "\n",
      "Validation result for Bernoulli NB BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 79.38%\n",
      "Model is 29.19% more accurate than null accuracy\n",
      "Train and test time: 259.98s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      8013\n",
      "           1       0.78      0.82      0.80      7954\n",
      "\n",
      "    accuracy                           0.79     15967\n",
      "   macro avg       0.79      0.79      0.79     15967\n",
      "weighted avg       0.79      0.79      0.79     15967\n",
      "\n",
      "Validation result for Adaboost AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "Null accuracy: 50.18%\n",
      "Accuracy: 70.23%\n",
      "Model is 20.05% more accurate than null accuracy\n",
      "Train and test time: 540.20s\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.62      0.68      8013\n",
      "           1       0.67      0.78      0.72      7954\n",
      "\n",
      "    accuracy                           0.70     15967\n",
      "   macro avg       0.71      0.70      0.70     15967\n",
      "weighted avg       0.71      0.70      0.70     15967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trgram_comparison = classifier_comparator(n_features=80000, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ridge Classifier', 0.8185632867789816, 580.59814620018),\n",
       " ('Logistic Regression', 0.8237615081104779, 611.4296820163727),\n",
       " ('Perceptron', 0.757562472599737, 750.1953251361847),\n",
       " ('Passive-Agressive Classifier', 0.7593160894344586, 186.06019163131714),\n",
       " ('Stochastic Gradient Descent', 0.8138660988288344, 183.67948579788208),\n",
       " ('LinearSVC', 0.8206300494770464, 827.0392413139343),\n",
       " ('L1 based LinearSVC', 0.82138159954907, 1220.1411337852478),\n",
       " ('KNN', 0.7187323855451869, 1968.039529800415),\n",
       " ('Nearest Centroid', 0.6378154944573182, 287.850460767746),\n",
       " ('Multinomial NB', 0.7973319972443164, 251.4556438922882),\n",
       " ('Bernoulli NB', 0.7937621344022046, 259.98261547088623),\n",
       " ('Adaboost', 0.7023235423060061, 540.1976461410522)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trgram_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Classifier', 'Accuracy', 'Time']\n",
    "df = pd.DataFrame(trgram_comparison, columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('trigramwithcountvectoriser.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
